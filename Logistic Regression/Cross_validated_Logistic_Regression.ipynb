{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation with chosen alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to add a bias term to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_bias_term(a):\n",
    "    \n",
    "    b = np.ones((a.shape[0],1))\n",
    "    a = np.hstack((b, a))\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate normalization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_params(a):\n",
    "    \n",
    "    mean = np.mean(a, axis=0)\n",
    "    standard_dev = np.std(a, axis=0)\n",
    "    \n",
    "    return mean, standard_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to perform normalization on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_feat(a, mean, standard_dev):\n",
    "    \n",
    "    a = (a - mean) / (standard_dev + 1e-8)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(a, Theta):\n",
    "    \n",
    "    b = 1.0/(1.0 + np.exp(-np.dot(a, Theta)))\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_func(a, Theta, h, thresh=0.50):\n",
    "    \n",
    "    b = h(a, Theta)\n",
    "    b = (b>thresh) * 1\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch gradient descent learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_learn(a, b, theta, h, alpha, iter_max=300):\n",
    "    iteration = 1\n",
    "    J_store = []\n",
    "    num = a.shape[0]\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        error = (h(a, theta) - b)\n",
    "        J_store.append(1.0/(2*num) * np.sum(error**2))\n",
    "        a_error = (a * error)\n",
    "        theta = theta - alpha * np.mean(a_error, axis=0)[:,np.newaxis]\n",
    "        \n",
    "        if iteration > iter_max:\n",
    "            break\n",
    "        iteration += 1\n",
    "        \n",
    "    return theta, J_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Pre-processed_Sarcasm_Headlines_Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply vectorizer to train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = True, analyzer='word',smooth_idf=True,use_idf=True)\n",
    "X_1 = vectorizer.fit_transform(dataset['headline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert split data to dataframes and 2-D arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26709, 5534)\n"
     ]
    }
   ],
   "source": [
    "X_1 = pd.DataFrame(X_1.toarray())\n",
    "print(X_1.shape)\n",
    "\n",
    "y_1 = dataset['is_sarcastic']\n",
    "y_1 = y_1[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, standard_dev = n_params(X_1)\n",
    "x_1_norm = n_feat(X_1, mean, standard_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending bias term to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1_norm_aug = append_bias_term(x_1_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create storage for error rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_error_train = []\n",
    "store_error_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform K-fold cross validation with chosen alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpill\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Index:  [ 5342  5343  5344 ... 26706 26707 26708] \n",
      "\n",
      "Test Index:  [   0    1    2 ... 5339 5340 5341]\n",
      "Fold: 1\n",
      "emprical error training(in %) = 10.072\n",
      "emprical error testing(in %) = 17.372\n",
      "Train Index:  [    0     1     2 ... 26706 26707 26708] \n",
      "\n",
      "Test Index:  [ 5342  5343  5344 ... 10681 10682 10683]\n",
      "Fold: 2\n",
      "emprical error training(in %) = 9.945\n",
      "emprical error testing(in %) = 16.979\n",
      "Train Index:  [    0     1     2 ... 26706 26707 26708] \n",
      "\n",
      "Test Index:  [10684 10685 10686 ... 16023 16024 16025]\n",
      "Fold: 3\n",
      "emprical error training(in %) = 10.268\n",
      "emprical error testing(in %) = 16.305\n",
      "Train Index:  [    0     1     2 ... 26706 26707 26708] \n",
      "\n",
      "Test Index:  [16026 16027 16028 ... 21365 21366 21367]\n",
      "Fold: 4\n",
      "emprical error training(in %) = 10.226\n",
      "emprical error testing(in %) = 16.698\n",
      "Train Index:  [    0     1     2 ... 21365 21366 21367] \n",
      "\n",
      "Test Index:  [21368 21369 21370 ... 26706 26707 26708]\n",
      "Fold: 5\n",
      "emprical error training(in %) = 9.832\n",
      "emprical error testing(in %) = 17.562\n"
     ]
    }
   ],
   "source": [
    "fold=1\n",
    "alpha=0.01\n",
    "cv = KFold(n_splits=5, random_state=100, shuffle=False)\n",
    "for train_index, test_index in cv.split(x_1_norm_aug):\n",
    "    \n",
    "    print(\"Train Index: \", train_index, \"\\n\")\n",
    "    print(\"Test Index: \", test_index)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = x_1_norm_aug[train_index], x_1_norm_aug[test_index], y_1[train_index], y_1[test_index]\n",
    "    \n",
    "    initial_thetas = np.zeros((X_train.shape[1],1))\n",
    "    learnt_thetas, J = param_learn(X_train, y_train, initial_thetas, h, alpha=alpha)\n",
    "    \n",
    "    train_predict = prediction_func(X_train, learnt_thetas, h)\n",
    "    test_predict = prediction_func(X_test, learnt_thetas, h)\n",
    "    \n",
    "    error_train = np.sum(y_train != train_predict) / train_predict.shape[0]\n",
    "    error_test = np.sum(y_test != test_predict) / test_predict.shape[0]\n",
    "    \n",
    "    store_error_train.append(error_train)\n",
    "    store_error_test.append(error_test)\n",
    "    \n",
    "    print('Fold: {}'.format(fold))\n",
    "    print('emprical error training(in %) = {:.3f}'.format(error_train*100))\n",
    "    print('emprical error testing(in %) = {:.3f}'.format(error_test*100))\n",
    "    fold+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print average of error percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validated training error: \n",
      "10.068705644594441\n",
      "Cross validated testing error: \n",
      "16.983061110230604\n"
     ]
    }
   ],
   "source": [
    "print('Cross validated training error: ')\n",
    "print(np.array(store_error_train).mean()*100)\n",
    "\n",
    "print('Cross validated testing error: ')\n",
    "print(np.array(store_error_test).mean()*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
